# ğŸ¤– InfoTech College Chatbot

A RAG (Retrieval-Augmented Generation) chatbot with a **beautiful web interface** built for InfoTech College of Business & IT.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.2+-orange.svg)

## âœ¨ Features

- ğŸ¨ **Web Chat Interface** - Beautiful, responsive chat UI
- ğŸ” **RAG Pipeline** - Retrieval-Augmented Generation for accurate responses
- ğŸš€ **FastAPI Backend** - High-performance REST API
- ğŸ¤– **Ollama LLM** - Local DeepSeek-r1:1.5b model
- ğŸ“Š **ChromaDB** - Vector database for semantic search
- ğŸ“„ **Multi-format** - Supports PDF, TXT, and Markdown files

---

## ğŸš€ Quick Start (5 Minutes)

### Prerequisites

1. **Python 3.10+** installed
2. **Ollama** installed from https://ollama.com/download

### Step 1: Clone & Enter Directory

```bash
git clone https://github.com/DPHeshanRanasinghe/ChatBot.git
cd ChatBot
```

### Step 2: Create Virtual Environment

```bash
# Using conda (recommended)
conda create -n chatbot python=3.10 -y
conda activate chatbot

# OR using venv
python -m venv venv
venv\Scripts\activate     # Windows
source venv/bin/activate  # Linux/Mac
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Setup Ollama Model

Open a **new terminal** and run:
```bash
ollama pull deepseek-r1:1.5b
ollama serve
```
Keep this terminal running!

### Step 5: Add Your Documents

Place your PDF, TXT, or MD files in the `docs/` folder. These will be the chatbot's knowledge base.

### Step 6: Run the Chatbot

```bash
python -m uvicorn chat_api:app --reload
```

### Step 7: Open the Website! ğŸ‰

Open your browser and go to:

ğŸ‘‰ **http://127.0.0.1:8000**

You'll see a beautiful chat interface where you can ask questions!

---

## âš¡ Daily Usage (Already Installed)

If you've already set up the project and just want to run it:

### 1. Open Terminal in the ChatBot folder

### 2. Activate your environment
```bash
# If using conda
conda activate chatbot

# If using venv (Windows)
venv\Scripts\activate
```

### 3. Make sure Ollama is running
```bash
ollama serve
```

### 4. Start the server
```bash
python -m uvicorn chat_api:app --reload
```

### 5. Open http://localhost:8000 in your browser

**That's it!** ğŸ‰

---

## ğŸ“¸ What You'll See

When you open http://localhost:8000, you'll get:

- ğŸ’¬ **Chat Window** - Type questions and get AI responses
- ğŸ¯ **Quick Buttons** - Pre-made questions to try
- âŒ¨ï¸ **Input Box** - Type your own questions
- ğŸ¤– **AI Assistant** - Answers based on your documents

---

## ğŸ® Two Ways to Use

| Mode | Command | Access |
|------|---------|--------|
| **Web UI** â­ | `python -m uvicorn chat_api:app --reload` | http://localhost:8000 |
| **Terminal** | `python main.py` | Type in terminal |
| **API Docs** | (after starting server) | http://localhost:8000/docs |

---

## ğŸ“ Project Structure

```
ChatBot/
â”œâ”€â”€ chat_api.py          # FastAPI REST API
â”œâ”€â”€ main.py              # CLI interface
â”œâ”€â”€ config.py            # Configuration
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html       # Web chat interface â­
â”œâ”€â”€ lib/                 # Core modules
â”œâ”€â”€ docs/                # Your documents go here ğŸ“„
â””â”€â”€ chromadb/            # Vector database (auto-created)
```

---

## âš™ï¸ Configuration

Edit `.env` file or `config.py` to customize:

| Setting | Default | Description |
|---------|---------|-------------|
| `LLM_MODEL` | `deepseek-r1:1.5b` | Ollama model |
| `CHUNK_SIZE` | `1000` | Document chunk size |
| `RETRIEVER_K` | `4` | Number of docs to retrieve |

---

## ğŸ› ï¸ Troubleshooting

### "Connection refused" error
â†’ Make sure Ollama is running: `ollama serve`

### "Model not found" error
â†’ Pull the model: `ollama pull deepseek-r1:1.5b`

### Port already in use
â†’ Use a different port: `python -m uvicorn chat_api:app --port 8001`

### Slow first response
â†’ Normal! First time loads the model into memory.

---

## ğŸ“š Tech Stack

- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
- [LangChain](https://langchain.com/) - LLM framework
- [Ollama](https://ollama.com/) - Local LLM
- [ChromaDB](https://www.trychroma.com/) - Vector database
- [HuggingFace](https://huggingface.co/) - Embeddings

---

## ğŸ‘¤ Author

**Heshan Ranasinghe**
- GitHub: [@DPHeshanRanasinghe](https://github.com/DPHeshanRanasinghe)

---

## ğŸ“„ License

MIT License - feel free to use and modify!

